{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_ipthyhdGfu"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4TuiaOj8b7qQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling2D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qC9q3lEdQZp"
      },
      "source": [
        "## Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZdrRsaPVcuqK"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load data from JSON file\n",
        "with open(\"/content/intents.json\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Initialize lists to hold training sentences, labels, and responses\n",
        "training_sentences = []\n",
        "training_labels = []\n",
        "labels = []\n",
        "responses = []\n",
        "\n",
        "# Iterate through each intent in the JSON data\n",
        "for intent in data[\"intents\"]:\n",
        "    # Add each pattern to the training sentences and corresponding tag to the training labels\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        training_sentences.append(pattern)\n",
        "        training_labels.append(intent[\"tag\"])\n",
        "\n",
        "    # Add the responses to the responses list\n",
        "    responses.append(intent[\"responses\"])\n",
        "\n",
        "    # If the tag is not already in labels, add it\n",
        "    if intent[\"tag\"] not in labels:\n",
        "        labels.append(intent[\"tag\"])\n",
        "\n",
        "# Determine the number of unique classes\n",
        "num_classes = len(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRvgpdLLgQIQ"
      },
      "source": [
        "## Encoding Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VldvCsSHgVEA"
      },
      "outputs": [],
      "source": [
        "# Initialize and fit the label encoder\n",
        "lbl_encoder = LabelEncoder()\n",
        "lbl_encoder.fit(training_labels)\n",
        "\n",
        "# Transform the training labels to numerical values\n",
        "training_labels = lbl_encoder.transform(training_labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5emd2pCiidjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be48c9d9-8829-4bfc-c01d-61fc1aa6e8cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 20, 16)            16000     \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 16)                0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                272       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8)                 136       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16408 (64.09 KB)\n",
            "Trainable params: 16408 (64.09 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "2/2 [==============================] - 2s 185ms/step - loss: 2.0772 - accuracy: 0.2121\n",
            "Epoch 2/20\n",
            "2/2 [==============================] - 0s 289ms/step - loss: 2.0752 - accuracy: 0.2121\n",
            "Epoch 3/20\n",
            "2/2 [==============================] - 1s 360ms/step - loss: 2.0736 - accuracy: 0.2121\n",
            "Epoch 4/20\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2.0723 - accuracy: 0.2121\n",
            "Epoch 5/20\n",
            "2/2 [==============================] - 0s 361ms/step - loss: 2.0711 - accuracy: 0.2121\n",
            "Epoch 6/20\n",
            "2/2 [==============================] - 0s 280ms/step - loss: 2.0700 - accuracy: 0.2121\n",
            "Epoch 7/20\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.0686 - accuracy: 0.2121\n",
            "Epoch 8/20\n",
            "2/2 [==============================] - 0s 188ms/step - loss: 2.0675 - accuracy: 0.2121\n",
            "Epoch 9/20\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.0667 - accuracy: 0.2121\n",
            "Epoch 10/20\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.0658 - accuracy: 0.2121\n",
            "Epoch 11/20\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.0651 - accuracy: 0.2121\n",
            "Epoch 12/20\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.0643 - accuracy: 0.2121\n",
            "Epoch 13/20\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.0634 - accuracy: 0.2121\n",
            "Epoch 14/20\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.0626 - accuracy: 0.2121\n",
            "Epoch 15/20\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.0620 - accuracy: 0.2121\n",
            "Epoch 16/20\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.0613 - accuracy: 0.2121\n",
            "Epoch 17/20\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.0607 - accuracy: 0.2121\n",
            "Epoch 18/20\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.0600 - accuracy: 0.2121\n",
            "Epoch 19/20\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.0592 - accuracy: 0.2121\n",
            "Epoch 20/20\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.0581 - accuracy: 0.2121\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the intents JSON file\n",
        "with open(\"/content/intents.json\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Extract training data\n",
        "training_sentences = []\n",
        "training_labels = []\n",
        "labels = []\n",
        "responses = []\n",
        "\n",
        "for intent in data[\"intents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        training_sentences.append(pattern)\n",
        "        training_labels.append(intent[\"tag\"])\n",
        "    responses.append(intent[\"responses\"])\n",
        "\n",
        "    if intent[\"tag\"] not in labels:\n",
        "        labels.append(intent[\"tag\"])\n",
        "\n",
        "# Encode the labels\n",
        "lbl_encoder = LabelEncoder()\n",
        "lbl_encoder.fit(training_labels)\n",
        "training_labels = lbl_encoder.transform(training_labels)\n",
        "\n",
        "# Tokenize the sentences\n",
        "vocab_size = 1000  # Define the size of the vocabulary\n",
        "embedding_dim = 16  # Define the dimension of the dense embedding\n",
        "max_len = 20  # Define the maximum length of input sequences\n",
        "oov_token = \"<OOV>\"  # Token for out-of-vocabulary words\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "padded_sequences = pad_sequences(sequences, truncating='post', maxlen=max_len)\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model.add(GlobalAveragePooling1D())\n",
        "model.add(Dense(16, activation=\"relu\"))\n",
        "model.add(Dense(len(labels), activation=\"softmax\"))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "# Define number of epochs for training\n",
        "epochs = 20\n",
        "\n",
        "# Fit the model on the training data\n",
        "history = model.fit(padded_sequences, np.array(training_labels), epochs=epochs)\n",
        "\n",
        "# Save the model and tokenizers\n",
        "model.save(\"chat\")\n",
        "\n",
        "with open(\"tokenizer.pickle\", \"wb\") as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(\"label_encoder.pickle\", \"wb\") as enc_file:\n",
        "    pickle.dump(lbl_encoder, enc_file, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJJOG2FItRJW"
      },
      "source": [
        "## Saving Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NeIzjgUVtVP8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"chat\")\n",
        "\n",
        "# Save the tokenizer to a file using pickle\n",
        "with open(\"tokenizer.pickle\", \"wb\") as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Save the label encoder to a file using pickle\n",
        "with open(\"label_encoder.pickle\", \"wb\") as ecn_file:\n",
        "    pickle.dump(lbl_encoder, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatBot Conversation"
      ],
      "metadata": {
        "id": "S4k0N8oEytqk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sTp-TaFouyk5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b6bfbed-56ab-4f1c-d4f5-01a57e9f5514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install colorama\n",
        "\n",
        "import colorama\n",
        "\n",
        "colorama.init()\n",
        "\n",
        "from colorama import Fore, Style, Back\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras as keras\n",
        "import pickle\n",
        "from colorama import Fore, Style\n",
        "import random\n",
        "\n",
        "# Load the intents JSON file\n",
        "with open(\"/content/intents.json\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "def chat():\n",
        "    # Load the trained model\n",
        "    model = keras.models.load_model(\"chat\")\n",
        "\n",
        "    # Load the tokenizer\n",
        "    with open(\"tokenizer.pickle\", \"rb\") as handle:\n",
        "        tokenizer = pickle.load(handle)\n",
        "\n",
        "    # Load the label encoder\n",
        "    with open(\"label_encoder.pickle\", \"rb\") as enc:\n",
        "        lbl_encoder = pickle.load(enc)\n",
        "\n",
        "    max_len = 20\n",
        "\n",
        "    while True:\n",
        "        # Get user input\n",
        "        print(Fore.LIGHTBLUE_EX + \"User: \" + Style.RESET_ALL, end=\"\")\n",
        "        inp = input()\n",
        "\n",
        "        if inp.lower() == \"quit\":\n",
        "            break\n",
        "\n",
        "        # Tokenize and pad the input\n",
        "        sequences = tokenizer.texts_to_sequences([inp])\n",
        "        padded_sequences = keras.preprocessing.sequence.pad_sequences(sequences, truncating=\"post\", maxlen=max_len)\n",
        "\n",
        "        # Predict the intent\n",
        "        result = model.predict(padded_sequences)\n",
        "        tag = lbl_encoder.inverse_transform([np.argmax(result)])[0]\n",
        "\n",
        "        # Debugging: print the predicted tag\n",
        "        print(f\"Predicted tag: {tag}\")\n",
        "\n",
        "        # Find and print the appropriate response\n",
        "        for i in data[\"intents\"]:\n",
        "            if i[\"tag\"] == tag:\n",
        "                response = random.choice(i[\"responses\"])\n",
        "                print(Fore.GREEN + \"ChatBot: \" + Style.RESET_ALL, response)\n",
        "                break\n",
        "\n",
        "        print(Fore.YELLOW + \"welcome messages\" + Style.RESET_ALL)\n",
        "\n",
        "# Start the chat\n",
        "chat()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMOgXDkw7CaL",
        "outputId": "1d6a8c83-dbb7-4f64-df7d-44d4ee5a8c8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: hi\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "Predicted tag: help\n",
            "ChatBot:  Tell me how can assist you\n",
            "welcome messages\n",
            "User: have a complaint\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted tag: help\n",
            "ChatBot:  Yes Sure, How can I support you\n",
            "welcome messages\n",
            "User: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}